{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ2xkl4YoY/rHhzW3X/jwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluehood/Transformer-Translation/blob/main/Transformer_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Transformer Architecture for Language Translation\n",
        "\n",
        "This notebook implements a neural machine translation system based on the groundbreaking paper \"Attention is All You Need\" (Vaswani et al., 2017). The Transformer architecture introduced in this paper revolutionized natural language processing by eliminating the need for recurrent or convolutional neural networks, instead relying entirely on attention mechanisms to capture relationships between words.\n",
        "\n",
        "Our implementation focuses on three key innovations from the paper:\n",
        "\n",
        "1. **Multi-Head Self-Attention**: Allowing the model to simultaneously attend to information from different representation subspaces\n",
        "2. **Encoder-Decoder Architecture**: Processing the input sequence and generating the output sequence using stacked attention layers\n",
        "3. **Positional Encoding**: Incorporating sequence order information without recurrence\n",
        "\n",
        "Through this project, we'll:\n",
        "- Implement the core components of the Transformer architecture\n",
        "- Train a model for English-French translation\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "The purpose of the notebook is to implement the Transformer architecture for language translation, discuss how the model functions and to understand the Pytorch implementation. This notebook is not designed to be used to train your own GPT model (although it could be modified to do this). If you want to train the full model please see the training details in the following repository: https://github.com/bluehood/Transformer-Translation.  "
      ],
      "metadata": {
        "id": "k3FMTF2JxJl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "AjKkVH_l_YOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture\n",
        "The Transformer consists of an encoder that processes the input sequence and a decoder that generates the translation.\n",
        "\n",
        "The transformer architecture represents a fundamental shift in how we approach sequence-to-sequence tasks like translation. Instead of processing text word by word like traditional models, it looks at the entire sequence at once.\n",
        "\n",
        "We'll give a brief overview of the structure of the Transformer. For a more detailed discussion, please see our implementation of a GPT: https://colab.research.google.com/github/bluehood/GPT-Implementation/blob/main/GPT_Implementation.ipynb.\n",
        "\n"
      ],
      "metadata": {
        "id": "13n-8h4x2M5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation\n",
        "The words in our languages need to be converted to a numerical representation before they are fed into the model. The most natural choice is word-level tokenisation, however, it is not the most performant choice (see the GPT implementation for a more thorough discussion).\n",
        "\n",
        "We define a basic word-level tokeniser which will be used for both languages."
      ],
      "metadata": {
        "id": "6PBhxsWi6nZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_tokenize(text):\n",
        "    # Convert to lowercase for consistency\n",
        "    text = text.lower()\n",
        "\n",
        "    # Add spaces around punctuation so they become separate tokens\n",
        "    text = re.sub(r'([.,!?;])', r' \\1 ', text)\n",
        "    text = re.sub(r'([\"\\'])', r' \\1 ', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters (except allowed punctuation)\n",
        "    text = re.sub(r'[^a-z0-9.,!?;\\'\\\" ]', ' ', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    # Split into individual tokens\n",
        "    tokens = text.split()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "KoX6_x847SxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs several crucial preprocessing steps:\n",
        "\n",
        "- Converting to lowercase: This reduces vocabulary size by treating \"Word\" and \"word\" as the same token\n",
        "- Handling punctuation: By adding spaces around punctuation marks, we treat them as separate tokens, which helps the model learn their grammatical significance\n",
        "- Cleaning text: Removing unusual characters helps standardize the input\n",
        "- Splitting into tokens: The final step creates a list of individual words and punctuation marks\n",
        "\n",
        "Once this is done we need to create the vocabulary, which is a mapping between words and their integer representations in the model."
      ],
      "metadata": {
        "id": "w-eLhspK7Woq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(tokens, min_frequency=2):\n",
        "    # Count token frequencies\n",
        "    token_counts = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        token_counts[token] += 1\n",
        "\n",
        "    # Start with special tokens\n",
        "    vocab = {\n",
        "        '<PAD>': 0,  # Used for padding shorter sequences\n",
        "        '<UNK>': 1,  # Used for unknown words\n",
        "        '<START>': 2,  # Marks sequence start\n",
        "        '<END>': 3,   # Marks sequence end\n",
        "    }\n",
        "\n",
        "    # Add frequent tokens to vocabulary\n",
        "    token_idx = len(vocab)\n",
        "    for token, count in token_counts.items():\n",
        "        if count >= min_frequency:\n",
        "            vocab[token] = token_idx\n",
        "            token_idx += 1\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "Qr8Zu5-F7WNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function:\n",
        "- Counts how often each token appears in the training data\n",
        "- Adds special tokens that serve specific purposes\n",
        "- Includes only tokens that appear at least min_frequency times, helping reduce vocabulary size and prevent overfitting on rare words\n"
      ],
      "metadata": {
        "id": "VwGmCP-_7x0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer\n",
        "Once we have our tokens converted to indices, the embedding layer transforms these indices into dense vectors that capture semantic meaning. This is implemented in the larger `Transformer` class which will be introduced as code later:\n",
        "\n",
        "```python\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        # Create separate embedding layers for source and target languages\n",
        "        self.src_tok_emb = nn.Embedding(config.src_vocab_size, config.n_embd)\n",
        "        self.tgt_tok_emb = nn.Embedding(config.tgt_vocab_size, config.n_embd)\n",
        "        \n",
        "        # Create positional embeddings\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "```\n",
        "Firstly, each token index is converted into a vector of size `n_embd`. These vectors are learned during training and end up capturing semantic relationships - similar words will have similar embeddings.\n",
        "\n",
        "Since the transformer has no inherent way of understanding word order, we add positional information with the positional embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "wM1U6Jr575Mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention\n",
        "The transformer uses attention to weigh the importance of different words when processing each word in a sequence. The attention mechanism is defined in our `MultiHeadAttention` class.\n",
        "\n",
        "- Each word is projected into three different vectors: query, key, and value\n",
        "- The query vector of each word is compared with the key vectors of all words to determine attention weights\n",
        "- These weights are used to create a weighted sum of the value vectors"
      ],
      "metadata": {
        "id": "-DH-4LUU6mgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NodMOINhwoWq"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.num_heads == 0\n",
        "\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_size = config.n_embd // config.num_heads\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.q_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.k_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.v_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.out_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, q, k=None, v=None, mask=None, is_causal=False):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        if k is None:\n",
        "            k = q\n",
        "        if v is None:\n",
        "            v = q\n",
        "\n",
        "        q = self.q_proj(q)\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "\n",
        "        if is_causal:\n",
        "            seq_len = q.size(-2)\n",
        "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=q.device), diagonal=1)\n",
        "            scores.masked_fill_(causal_mask, float('-inf'))\n",
        "\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 3:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            scores.masked_fill_(~mask, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ v\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.n_embd)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Encoder: Understanding the Input\n",
        "\n",
        "The encoder's job is to process the input sentence and create a rich representation that captures both the meaning of each word and its relationships with other words. It does this through multiple layers, each containing two sub-components: the Multi-Head attention layers and feed-forward layers. After each layer, layer normalisation is applied."
      ],
      "metadata": {
        "id": "SnorNWRk3wr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention followed by feed-forward processing\n",
        "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), mask)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return self.ln_f(x)"
      ],
      "metadata": {
        "id": "d6gnQsJj4Hyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each encoder block:\n",
        "\n",
        "- First apply self-attention, allowing each word to gather information from other relevant words\n",
        "- Then processes this information through a feed-forward network\n",
        "- Uses residual connections (the `x + ...` parts) to maintain a smooth information flow\n",
        "- Applies layer normalization (`ln1` and `ln2`) to stabilise the learning process\n",
        "\n",
        "Typically we have several Encoder blocks applied in sequence; in the original paper, six encoder blocks were used."
      ],
      "metadata": {
        "id": "YBEyNshB4Iz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Decoder: Generating the Translation\n",
        "\n",
        "The decoder has the complex task of generating the translation one word at a time. It needs to:\n",
        "\n",
        "- Look at the translation it has generated so far\n",
        "- Consider the entire input sentence\n",
        "- Decide what word comes next\n",
        "\n",
        "This is implemented through the following class:"
      ],
      "metadata": {
        "id": "M7E-_G5I4jAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionBlock(nn.Module):\n",
        "    \"\"\"Transformer block with cross-attention to encoder output\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.self_attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = MultiHeadAttention(config)\n",
        "        self.ln3 = nn.LayerNorm(config.n_embd)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, enc_out, self_mask=None, cross_mask=None):\n",
        "        # Self attention with causal masking\n",
        "        x = x + self.self_attn(\n",
        "            self.ln1(x),\n",
        "            mask=self_mask,\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # Cross attention to encoder output\n",
        "        x = x + self.cross_attn(\n",
        "            q=self.ln2(x),\n",
        "            k=enc_out,\n",
        "            v=enc_out,\n",
        "            mask=cross_mask\n",
        "        )\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Pre-cross attention transformer blocks\n",
        "        self.pre_blocks = nn.ModuleList([\n",
        "            Block(config) for _ in range(config.n_pre_cross_layer)\n",
        "        ])\n",
        "        # Cross attention blocks\n",
        "        self.cross_blocks = nn.ModuleList([\n",
        "            CrossAttentionBlock(config) for _ in range(config.n_cross_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, enc_out, padding_mask=None, cross_mask=None):\n",
        "        # First run through pre-cross attention blocks with causal masking\n",
        "        for block in self.pre_blocks:\n",
        "            x = block(x, padding_mask, is_causal=True)\n",
        "\n",
        "        # Then through cross attention blocks\n",
        "        # Cross attention blocks still use causal masking for self-attention\n",
        "        for block in self.cross_blocks:\n",
        "            x = block(x, enc_out, padding_mask, cross_mask)\n",
        "\n",
        "        return self.ln_f(x)\n",
        "\n",
        "        # Feed forward\n",
        "        x = x + self.ffwd(self.ln3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "kVDvvj1p4rwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder uses two types of attention:\n",
        "\n",
        "- Masked self-attention to look at the previously generated word. We use masking to ensure that the model does not peek forward during training and essentially cheat by viewing the words it looking to translate\n",
        "- Cross-attention to incorporate the input sequence into its predictions\n",
        "- Feed-forward processing to combine this information\n",
        "\n",
        "The decoder typically has a stack before the input sequence representations are incorporated."
      ],
      "metadata": {
        "id": "9rKBtUug40CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it Together\n",
        "We can now define the whole model architecture in the `Transformer` class:"
      ],
      "metadata": {
        "id": "0gK7LBif5u1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Source and target embeddings\n",
        "        self.src_tok_emb = nn.Embedding(config.src_vocab_size, config.n_embd)\n",
        "        self.tgt_tok_emb = nn.Embedding(config.tgt_vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Encoder and Decoder\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "        # Output projection\n",
        "        self.head = nn.Linear(config.n_embd, config.tgt_vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src_ids, tgt_ids, src_mask=None, tgt_mask=None):\n",
        "        B, T = src_ids.size()\n",
        "\n",
        "        # Source embedding\n",
        "        src_emb = self.src_tok_emb(src_ids)\n",
        "        src_pos = self.pos_emb[:, :T, :]\n",
        "        x = self.drop(src_emb + src_pos)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = self.encoder(x, src_mask)\n",
        "\n",
        "        # Target embedding\n",
        "        tgt_emb = self.tgt_tok_emb(tgt_ids)\n",
        "        tgt_pos = self.pos_emb[:, :tgt_ids.size(1), :]\n",
        "        y = self.drop(tgt_emb + tgt_pos)\n",
        "\n",
        "        # Decode\n",
        "        y = self.decoder(y, encoder_out, tgt_mask, src_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.head(y)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, src_ids, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate translation tokens autoregressively\"\"\"\n",
        "        self.eval()\n",
        "        B, T = src_ids.size()\n",
        "\n",
        "        # Create source padding mask (1 for tokens, 0 for padding)\n",
        "        src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2).to(dtype=torch.bool)\n",
        "\n",
        "        # First encode the source sequence\n",
        "        src_emb = self.src_tok_emb(src_ids)\n",
        "        pos_emb = self.pos_emb[:, :T, :]\n",
        "        x = self.drop(src_emb + pos_emb)\n",
        "        encoder_out = self.encoder(x, src_mask)\n",
        "\n",
        "        # Initialize target sequence with START token\n",
        "        tgt_ids = torch.full((B, 1), fill_value=2, dtype=torch.long, device=src_ids.device)  # Assume 2 is START token\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Cut off if sequence is too long\n",
        "            if tgt_ids.size(1) > self.config.block_size:\n",
        "                break\n",
        "\n",
        "            # Create target padding mask (1 for tokens, 0 for padding)\n",
        "            tgt_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2).to(dtype=torch.bool)\n",
        "\n",
        "            # Get embeddings for target sequence\n",
        "            tgt_emb = self.tgt_tok_emb(tgt_ids)\n",
        "            pos_emb = self.pos_emb[:, :tgt_ids.size(1), :]\n",
        "            y = self.drop(tgt_emb + pos_emb)\n",
        "\n",
        "            # Decode\n",
        "            y = self.decoder(y, encoder_out, tgt_mask, src_mask)\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = self.head(y)\n",
        "\n",
        "            # Only take the last token's logits\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Optional top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append next token to sequence\n",
        "            tgt_ids = torch.cat((tgt_ids, next_token), dim=1)\n",
        "\n",
        "            # Stop if we hit the EOS token (assume 3 is EOS token)\n",
        "            if (next_token == 3).any():\n",
        "                break\n",
        "\n",
        "        return tgt_ids\n",
        "\n",
        "class TransformerConfig:\n",
        "    \"\"\"Configuration class to store the configuration of a `Transformer`.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size=50257,\n",
        "        tgt_vocab_size=50257,\n",
        "        block_size=1024,\n",
        "        n_layer=12,  # Number of encoder layers\n",
        "        n_pre_cross_layer=6,  # Number of decoder layers before cross attention\n",
        "        n_cross_layer=6,  # Number of decoder layers with cross attention\n",
        "        n_embd=768,\n",
        "        num_heads=12,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_pre_cross_layer = n_pre_cross_layer\n",
        "        self.n_cross_layer = n_cross_layer\n",
        "        self.n_embd = n_embd\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout"
      ],
      "metadata": {
        "id": "LuOI_7tK6Oso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "We used the following dataset of English to French translations: https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench.\n",
        "\n",
        "## Preparing the Data\n",
        "Before we can train the model, we need to prepare our data in a way that the model can process. We define the `TranslationDataset` to tokenise the dataset and return the English to French pairs at a given index in the dataset:"
      ],
      "metadata": {
        "id": "bA_8uOOz9fEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer, max_length=128):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Pre-tokenize all texts\n",
        "        print(\"Pre-tokenizing source texts...\")\n",
        "        self.src_encoded = tokenize_and_pad(src_texts, src_tokenizer, max_length)\n",
        "        print(\"Pre-tokenizing target texts...\")\n",
        "        self.tgt_encoded = tokenize_and_pad(tgt_texts, tgt_tokenizer, max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'src_ids': self.src_encoded[idx],\n",
        "            'tgt_ids': self.tgt_encoded[idx],\n",
        "            'src_text': self.src_texts[idx],\n",
        "            'tgt_text': self.tgt_texts[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "65vl7ZQd-Mcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is implemented by the `train_model` sequence. We'll discuss a few important steps before defining this function:\n",
        "\n",
        "### Learning Rate Management\n",
        "Just as a language learner might need to adjust their learning pace, our model uses a learning rate scheduler:\n",
        "\n",
        "```python\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        " optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        ")\n",
        "```\n",
        "\n",
        "This scheduler watches the model's performance and adjusts the learning rate when improvement slows down. If the model stops improving for a while (defined by the patience parameter), the learning rate is reduced by half, allowing for finer adjustments to the model's parameters.\n",
        "\n",
        "### Validation and Model Selection\n",
        "During training, we regularly check how well the model is doing on unseen data:\n",
        "```python\n",
        "if avg_val_loss < best_val_loss:\n",
        " best_val_loss = avg_val_loss\n",
        " torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': best_val_loss,\n",
        "    }, './models/best_model.pt')\n",
        "```\n",
        "\n",
        "This is like giving the language learner a test on material they haven't specifically studied. We save the version of the model that performs best on these validation tests, as this indicates good generalization to new sentences.\n",
        "\n",
        "### Training Configuration\n",
        "Our model uses specific hyperparameters chosen to balance training speed and performance:\n",
        "```python\n",
        "config = TransformerConfig(\n",
        " src_vocab_size=len(eng_tokenizer),\n",
        " tgt_vocab_size=len(fr_tokenizer),\n",
        " block_size=128,\n",
        " n_layer=6,\n",
        " n_pre_cross_layer=3,\n",
        " n_cross_layer=3,\n",
        " n_embd=256,\n",
        " num_heads=8,\n",
        " dropout=0.1\n",
        ")\n",
        "```\n",
        "These parameters define:\n",
        "\n",
        "- The size of our vocabularies for both languages\n",
        "- The maximum sequence length (block_size)\n",
        "- The model's architecture (number of layers and their sizes)\n",
        "- Regularisation strength (dropout)\n",
        "\n",
        "### Preventing Overfitting\n",
        "The training process includes several mechanisms to prevent the model from memorizing the training data instead of learning to translate:\n",
        "\n",
        "- Dropout is applied throughout the model:\n",
        "```python\n",
        "self.drop = nn.Dropout(config.dropout)\n",
        "```\n",
        "- The training data is reshuffled at the start of each epoch:\n",
        "```python\n",
        "reshuffle_training_data(train_dataset)\n",
        "```\n",
        "\n",
        "- Gradient clipping prevents extreme parameter updates:\n",
        "```python\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "```\n",
        "\n",
        "The full training loop can be implemented as:"
      ],
      "metadata": {
        "id": "eng9NvJG-O6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(texts, tokenizer, max_length):\n",
        "    \"\"\"Tokenize and pad a list of texts in one batch operation\"\"\"\n",
        "    encoded_texts = []\n",
        "    for text in texts:\n",
        "        # Encode text\n",
        "        encoded = [tokenizer['<START>']] + \\\n",
        "                 [tokenizer.get(token, tokenizer['<UNK>'])\n",
        "                  for token in text.split()] + \\\n",
        "                 [tokenizer['<END>']]\n",
        "\n",
        "        # Truncate if necessary\n",
        "        encoded = encoded[:max_length]\n",
        "\n",
        "        # Pad sequence\n",
        "        encoded += [tokenizer['<PAD>']] * (max_length - len(encoded))\n",
        "        encoded_texts.append(encoded)\n",
        "\n",
        "    return torch.tensor(encoded_texts)\n",
        "\n",
        "def create_masks(src_ids, tgt_ids):\n",
        "    # Source mask (padding mask)\n",
        "    src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, src_len)\n",
        "\n",
        "    # Target mask (combination of padding and subsequent mask)\n",
        "    tgt_pad_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, tgt_len)\n",
        "\n",
        "    tgt_len = tgt_ids.size(1)\n",
        "    subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt_ids.device, dtype=torch.bool))\n",
        "    subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(1)  # (1, 1, tgt_len, tgt_len)\n",
        "\n",
        "    tgt_mask = tgt_pad_mask & subsequent_mask\n",
        "\n",
        "    return src_mask.to(torch.bool), tgt_mask.to(torch.bool)\n",
        "\n",
        "def load_tokenizer(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_dataset(path):\n",
        "    english_sentences = []\n",
        "    french_sentences = []\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip header\n",
        "        for row in csv_reader:\n",
        "            if len(row) == 2:\n",
        "                english_sentences.append(row[0].lower())  # Convert to lowercase\n",
        "                french_sentences.append(row[1].lower())  # Convert to lowercase\n",
        "\n",
        "    return english_sentences, french_sentences\n",
        "\n",
        "def reshuffle_training_data(dataset):\n",
        "    \"\"\"Reshuffle the training data while keeping pairs aligned\"\"\"\n",
        "    indices = list(range(len(dataset.src_texts)))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Reorder all dataset attributes using the shuffled indices\n",
        "    dataset.src_texts = [dataset.src_texts[i] for i in indices]\n",
        "    dataset.tgt_texts = [dataset.tgt_texts[i] for i in indices]\n",
        "    dataset.src_encoded = dataset.src_encoded[indices]\n",
        "    dataset.tgt_encoded = dataset.tgt_encoded[indices]\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, train_dataset, num_epochs, device, learning_rate=3e-4):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Reshuffle training data at the start of each epoch\n",
        "        reshuffle_training_data(train_dataset)\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_pbar = tqdm(train_dataloader, desc=f'Epoch [{epoch+1}/{num_epochs}]', leave=False)\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            # Move batch to device\n",
        "            src_ids = batch['src_ids'].to(device)  # [batch_size, seq_len]\n",
        "            tgt_ids = batch['tgt_ids'].to(device)  # [batch_size, seq_len]\n",
        "\n",
        "            # Create attention masks\n",
        "            src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
        "            tgt_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, tgt_len]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(\n",
        "                src_ids=src_ids,\n",
        "                tgt_ids=tgt_ids[:, :-1],  # Remove last token from target input\n",
        "                src_mask=src_mask,\n",
        "                tgt_mask=tgt_mask[:, :, :, :-1]  # Adjust mask accordingly\n",
        "            )\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(\n",
        "                logits.contiguous().view(-1, logits.size(-1)),\n",
        "                tgt_ids[:, 1:].contiguous().view(-1),  # Shift target ids right by 1\n",
        "                ignore_index=0  # Ignore padding token\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc='Validation', leave=False):\n",
        "                src_ids = batch['src_ids'].to(device)\n",
        "                tgt_ids = batch['tgt_ids'].to(device)\n",
        "\n",
        "                src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "                tgt_mask = (tgt_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "                logits = model(\n",
        "                    src_ids=src_ids,\n",
        "                    tgt_ids=tgt_ids[:, :-1],\n",
        "                    src_mask=src_mask,\n",
        "                    tgt_mask=tgt_mask[:, :, :, :-1]\n",
        "                )\n",
        "\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.contiguous().view(-1, logits.size(-1)),\n",
        "                    tgt_ids[:, 1:].contiguous().view(-1),\n",
        "                    ignore_index=0\n",
        "                )\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] | Train Loss = {avg_train_loss:.4f} | Validation Loss = {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_val_loss,\n",
        "            }, './models/best_model.pt')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizers\n",
        "eng_tokenizer = load_tokenizer('./models/english_tokeniser.json')\n",
        "fr_tokenizer = load_tokenizer('./models/french_tokeniser.json')\n",
        "\n",
        "# Load dataset\n",
        "eng_sentences, fr_sentences = load_dataset('./data/eng_french.csv')\n",
        "\n",
        "# Create shuffled indices for the full dataset\n",
        "indices = list(range(len(eng_sentences)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Use the shuffled indices to reorder both sentence lists simultaneously\n",
        "eng_sentences = [eng_sentences[i] for i in indices]\n",
        "fr_sentences = [fr_sentences[i] for i in indices]\n",
        "\n",
        "# Create train/val split\n",
        "split_idx = int(len(eng_sentences) * 0.9)\n",
        "\n",
        "train_eng = eng_sentences[:split_idx]\n",
        "train_fr = fr_sentences[:split_idx]\n",
        "val_eng = eng_sentences[split_idx:]\n",
        "val_fr = fr_sentences[split_idx:]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TranslationDataset(\n",
        "    train_eng, train_fr, eng_tokenizer, fr_tokenizer\n",
        ")\n",
        "val_dataset = TranslationDataset(\n",
        "    val_eng, val_fr, eng_tokenizer, fr_tokenizer\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "config = TransformerConfig(\n",
        "    src_vocab_size=len(eng_tokenizer),\n",
        "    tgt_vocab_size=len(fr_tokenizer),\n",
        "    block_size=128,\n",
        "    n_layer=6,\n",
        "    n_pre_cross_layer=3,\n",
        "    n_cross_layer=3,\n",
        "    n_embd=256,\n",
        "    num_heads=8,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "model = Transformer(config).to(device)\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    train_dataset=train_dataset,  # Pass the training dataset for reshuffling\n",
        "    num_epochs=20,\n",
        "    device=device,\n",
        "    learning_rate=3e-4\n",
        ")"
      ],
      "metadata": {
        "id": "ieRCQECm_NWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}